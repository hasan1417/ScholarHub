# Ethical Considerations and Privacy

## 1. AI Hallucination Risk

ScholarHub integrates large language models (LLMs) for research assistance, including paper search, summarisation, and academic writing. LLMs are known to generate plausible but factually incorrect statements, a phenomenon commonly referred to as "hallucination." In an academic context, hallucinated citations or fabricated claims pose a direct threat to research integrity.

### Mitigations in Place

- **Server-side search cache.** Search results returned by the Discussion AI are stored in a Redis cache on the server (`search_cache.py`). When the model references papers from a search, the system resolves them from this cache rather than trusting client-supplied data. This prevents prompt-injection attacks where a malicious actor could craft paper titles or abstracts to manipulate the model.

- **Deterministic policy engine.** A rule-based policy engine (`policy.py`) classifies user intent before the LLM is invoked. When a user requests papers, the policy forces a `search_papers` tool call, ensuring the model retrieves results from real academic APIs (Semantic Scholar, OpenAlex, CORE, CrossRef, PubMed) rather than generating them from memory. This deterministic routing operates independently of the model's own judgement.

- **Grounded tool use.** The Discussion AI operates through a tool-calling architecture (`tool_orchestrator.py`). The system prompt instructs the model to use `\cite{}` commands only for papers that exist in search results or the project library, and to never fabricate references. Tool outputs are populated from external academic databases, not model generation.

### Acknowledged Gaps

- **No citation verification step.** The system does not independently verify that a citation key generated by the model corresponds to a real paper. If the model hallucinates a `\cite{authorYYYYword}` tag that was not returned by any tool call, no automated check currently catches this.

- **Abstract-level depth.** Search results contain abstracts only. The model may make claims about a paper's methodology or findings that are not supported by the abstract alone. Users are advised to ingest full-text PDFs into the library before relying on AI-generated analysis.

## 2. AI-Generated Content and Academic Integrity

ScholarHub provides tools that can generate LaTeX sections, literature reviews, and abstracts. These capabilities raise questions about authorship and academic honesty.

### Design Philosophy

The platform's AI writing tools are designed as **assistive, not substitutive**. The system prompt explicitly scopes the assistant's role to paraphrasing, explaining, suggesting, and structuring content rather than producing original research arguments. Generated LaTeX sections include citation commands that reference real papers in the user's library, encouraging engagement with primary sources.

### Limitations

- **No plagiarism detection.** ScholarHub does not integrate plagiarism-detection services (e.g., Turnitin, iThenticate). Users are responsible for verifying the originality of AI-assisted text before submission.

- **Institutional policy compliance.** Policies on AI-assisted writing vary across institutions and journals. Users should consult their institution's guidelines on acceptable use of AI tools in academic work.

## 3. Data Privacy

### Data Stored on the Server

| Data Type | Storage | Purpose |
|-----------|---------|---------|
| User accounts | PostgreSQL | Authentication, profile |
| Research papers and metadata | PostgreSQL | Library management |
| Discussion messages | PostgreSQL | Conversation history |
| Document drafts (LaTeX, rich text) | PostgreSQL + filesystem | Collaborative editing |
| Embeddings (pgvector) | PostgreSQL | Semantic search within project libraries |
| Search result cache | Redis (1-hour TTL) | Prompt-injection defence, result deduplication |
| Uploaded files (PDFs, images) | Filesystem (`uploads/`) | Document ingestion, reference storage |

### External Services Receiving User Content

| Service | Data Sent | Purpose |
|---------|-----------|---------|
| OpenAI API | Discussion messages, document content for summarisation/writing, text for embeddings | Chat completions, tool-calling orchestration, embedding generation |
| OpenRouter | Discussion messages, document content (when user selects an OpenRouter model) | Multi-model chat completions via user's own API key |
| Semantic Scholar, OpenAlex, CORE, CrossRef, PubMed | Search queries (derived from user messages) | Academic paper discovery |

**Third-party data processing.** OpenAI retains API inputs for up to 30 days for abuse monitoring. OpenRouter forwards requests to downstream providers whose retention policies vary. Users should consult each provider's data-use terms before transmitting sensitive research content.

### API Key Management

Users provide their own OpenRouter API keys through project settings. These keys are encrypted at rest using Fernet symmetric encryption (AES-128-CBC + HMAC-SHA256) via `app/core/encryption.py` before being stored in the database. Keys are decrypted only at the moment of use for API calls.

### Data Retention and Deletion

There is currently no self-service account or data deletion workflow. Users who wish to have their data removed must contact a project administrator. This is a known limitation; a future iteration should provide in-app deletion in line with data-minimisation principles.

### Cookie Policy

The application uses httpOnly cookies for refresh token storage and OAuth session state. No tracking or analytics cookies are set.

### Regulatory Awareness

ScholarHub does not currently implement formal GDPR compliance mechanisms. For EU-based users, a production deployment would need to include explicit consent collection, a published privacy policy, and Data Processing Agreements (DPAs) with third-party AI providers. These requirements are acknowledged as necessary prerequisites for any public-facing release.

## 4. Security Measures

### Authentication and Session Management

- **JWT-based authentication.** Access tokens are signed with HS256 and include standard claims (`exp`, `iat`, `iss`, `jti`). Access tokens expire after 60 minutes; refresh tokens expire after 7 days.
- **Refresh token rotation with reuse detection.** Each token refresh issues a new token and stores the hash of the previous one. If a previously rotated token is replayed, all tokens for the user are invalidated, mitigating token theft scenarios (`auth.py`, lines 309-321).
- **Bcrypt password hashing.** Passwords are hashed with bcrypt via the `passlib` library. Refresh tokens are stored as SHA-256 hashes, never in plaintext.
- **Google OAuth.** OAuth state parameters use cryptographically random tokens (`secrets.token_urlsafe`) for CSRF protection.

### Access Control

- **Role-based access control (RBAC) on AI tools.** The tool registry enforces a three-tier role hierarchy (viewer, editor, admin/owner). Tools are filtered before being sent to the LLM so the model never sees tools the user cannot access. Permission is checked again at execution time (fail-closed) via `permissions.py`.
- **Endpoint-level authentication.** All API endpoints (except health checks and public auth routes) require a valid JWT, enforced through FastAPI dependency injection (`deps.py`).
- **Email verification gates.** Sensitive operations require a verified email address, enforced by a `get_current_verified_user` dependency.

### Network and Rate Limiting

- **CORS.** Cross-Origin Resource Sharing is configured via an explicit allowlist (`BACKEND_CORS_ORIGINS` in `config.py`). In production, only the application domain is permitted.
- **Rate limiting.** Authentication endpoints are rate-limited using `slowapi`: registration at 5 requests/minute, login at 10 requests/minute, and password reset at 3 requests/minute (`auth.py`). A global default limit of 100 requests/minute is applied backend-wide (`config.py`).

### Prompt Injection Defence

- **Server-side search cache.** As described in Section 1, search results are cached server-side in Redis. The model references papers by a server-generated search ID, preventing clients from injecting fabricated paper data into the conversation context. This is a deterministic, code-level defence that operates regardless of model behaviour.
- **Scope guardrails (soft).** The system prompt instructs the assistant to limit itself to research-related tasks and to decline clearly out-of-scope requests (e.g., personal emails, coding homework). These are advisory prompt-level constraints and are not deterministically enforced; a sufficiently adversarial input may bypass them. They should be understood as best-effort guidance, distinct from the code-enforced cache defence above.
